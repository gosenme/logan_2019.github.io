---
title: 中文自然语言处理入门-3
---
<article id="topicContainer" class="column_content"><h2 class="topic_title"></h2><div><h3 id="">前言</h3>
<p>从本文开始，我们就要真正进入实战部分。首先，我们按照中文自然语言处理流程的第一步获取语料，然后重点进行中文分词的学习。中文分词有很多种，常见的比如有中科院计算所 NLPIR、哈工大 LTP、清华大学 THULAC 、斯坦福分词器、Hanlp 分词器、jieba 分词、IKAnalyzer 等。这里针对 jieba 和 HanLP 分别介绍不同场景下的中文分词应用。</p>
<h3 id="jieba">jieba 分词</h3>
<h4 id="jieba-1">jieba 安装</h4>
<p>（1）Python 2.x 下 jieba 的三种安装方式，如下：</p>
<ul>
<li><p><strong>全自动安装</strong>：执行命令 <code>easy_install jieba</code> 或者 <code>pip install jieba</code> / <code>pip3 install jieba</code>，可实现全自动安装。</p></li>
<li><p><strong>半自动安装</strong>：先<a href="https://pypi.python.org/pypi/jieba/">下载 jieba</a>，解压后运行 <code>python setup.py install</code>。</p></li>
<li><p><strong>手动安装</strong>：将 jieba 目录放置于当前目录或者 site-packages 目录。</p></li>
</ul>
<p>安装完通过 <code>import jieba</code> 验证安装成功与否。</p>
<p>（2）Python 3.x 下的安装方式。</p>
<p>Github 上 jieba 的 Python3.x 版本的路径是：https://github.com/fxsjy/jieba/tree/jieba3k。</p>
<p>通过 <code>git clone https://github.com/fxsjy/jieba.git</code> 命令下载到本地，然后解压，再通过命令行进入解压目录，执行 <code>python setup.py install</code> 命令，即可安装成功。</p>
<h4 id="jieba-2">jieba 的分词算法</h4>
<p>主要有以下三种：</p>
<ol>
<li>基于统计词典，构造前缀词典，基于前缀词典对句子进行切分，得到所有切分可能，根据切分位置，构造一个有向无环图（DAG）；</li>
<li>基于DAG图，采用动态规划计算最大概率路径（最有可能的分词结果），根据最大概率路径分词；</li>
<li>对于新词(词库中没有的词），采用有汉字成词能力的 HMM 模型进行切分。</li>
</ol>
<h4 id="jieba-3">jieba 分词</h4>
<p>下面我们进行 jieba 分词练习，第一步首先引入 jieba 和语料:</p>
<pre><code>    import jieba
    content = "现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。"
</code></pre>
<p>（1）<strong>精确分词</strong></p>
<p>精确分词：精确模式试图将句子最精确地切开，精确分词也是默认分词。</p>
<pre><code>segs_1 = jieba.cut(content, cut_all=False)
print("/".join(segs_1))
</code></pre>
<p>其结果为：</p>
<blockquote>
  <p>现如今/，/机器/学习/和/深度/学习/带动/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大成功/。</p>
</blockquote>
<p>（2）<strong>全模式</strong></p>
<p>全模式分词：把句子中所有的可能是词语的都扫描出来，速度非常快，但不能解决歧义。</p>
<pre><code>    segs_3 = jieba.cut(content, cut_all=True)
    print("/".join(segs_3))
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>现如今/如今///机器/学习/和/深度/学习/带动/动人/人工/人工智能/智能/飞速/的/发展///并/在/图片/处理///语音/识别/领域/取得/巨大/巨大成功/大成/成功//</p>
</blockquote>
<p>（3）<strong>搜索引擎模式</strong></p>
<p>搜索引擎模式：在精确模式的基础上，对长词再次切分，提高召回率，适合用于搜索引擎分词。 </p>
<pre><code>    segs_4 = jieba.cut_for_search(content)
    print("/".join(segs_4))
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>如今/现如今/，/机器/学习/和/深度/学习/带动/人工/智能/人工智能/飞速/的/发展/，/并/在/图片/处理/、/语音/识别/领域/取得/巨大/大成/成功/巨大成功/。</p>
</blockquote>
<p>（4）<strong>用 lcut 生成 list</strong></p>
<p>jieba.cut 以及 <code>jieba.cut_for_search</code> 返回的结构都是一个可迭代的 Generator，可以使用 for 循环来获得分词后得到的每一个词语（Unicode）。jieba.lcut 对 cut 的结果做了封装，l 代表 list，即返回的结果是一个 list 集合。同样的，用 <code>jieba.lcut_for_search</code> 也直接返回 list 集合。</p>
<pre><code>    segs_5 = jieba.lcut(content)
    print(segs_5)
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>['现如今', '，', '机器', '学习', '和', '深度', '学习', '带动', '人工智能', '飞速', '的', '发展', '，', '并', '在', '图片', '处理', '、', '语音', '识别', '领域', '取得', '巨大成功', '。']</p>
</blockquote>
<p>（5）<strong>获取词性</strong></p>
<p>jieba 可以很方便地获取中文词性，通过 jieba.posseg 模块实现词性标注。</p>
<pre><code>    import jieba.posseg as psg
    print([(x.word,x.flag) for x in psg.lcut(content)])
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>[('现如今', 't'), ('，', 'x'), ('机器', 'n'), ('学习', 'v'), ('和', 'c'), ('深度', 'ns'), ('学习', 'v'), ('带动', 'v'), ('人工智能', 'n'), ('飞速', 'n'), ('的', 'uj'), ('发展', 'vn'), ('，', 'x'), ('并', 'c'), ('在', 'p'), ('图片', 'n'), ('处理', 'v'), ('、', 'x'), ('语音', 'n'), ('识别', 'v'), ('领域', 'n'), ('取得', 'v'), ('巨大成功', 'nr'), ('。', 'x')]</p>
</blockquote>
<p>（6）<strong>并行分词</strong> </p>
<p>并行分词原理为文本按行分隔后，分配到多个 Python 进程并行分词，最后归并结果。 </p>
<p>用法： </p>
<pre><code>jieba.enable_parallel(4) # 开启并行分词模式，参数为并行进程数 。
jieba.disable_parallel() # 关闭并行分词模式 。
</code></pre>
<p>注意： 并行分词仅支持默认分词器 jieba.dt 和 jieba.posseg.dt。目前暂不支持 Windows。</p>
<p>（7）<strong>获取分词结果中词列表的 top n</strong></p>
<pre><code>    from collections import Counter
    top5= Counter(segs_5).most_common(5)
    print(top5)
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>[('，', 2), ('学习', 2), ('现如今', 1), ('机器', 1), ('和', 1)]</p>
</blockquote>
<p>（8）<strong>自定义添加词和字典</strong></p>
<p>默认情况下，使用默认分词，是识别不出这句话中的“铁甲网”这个新词，这里使用用户字典提高分词准确性。</p>
<pre><code>    txt = "铁甲网是中国最大的工程机械交易平台。"
    print(jieba.lcut(txt))
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>['铁甲', '网是', '中国', '最大', '的', '工程机械', '交易平台', '。']</p>
</blockquote>
<p>如果添加一个词到字典，看结果就不一样了。</p>
<pre><code>    jieba.add_word("铁甲网")
    print(jieba.lcut(txt))
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>['铁甲网', '是', '中国', '最大', '的', '工程机械', '交易平台', '。']</p>
</blockquote>
<p>但是，如果要添加很多个词，一个个添加效率就不够高了，这时候可以定义一个文件，然后通过 <code>load_userdict()</code>函数，加载自定义词典，如下：</p>
<pre><code>    jieba.load_userdict('user_dict.txt')
    print(jieba.lcut(txt))
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>['铁甲网', '是', '中国', '最大', '的', '工程机械', '交易平台', '。']</p>
</blockquote>
<p><strong>注意事项：</strong></p>
<p>jieba.cut 方法接受三个输入参数: 需要分词的字符串；<code>cut_all</code> 参数用来控制是否采用全模式；HMM 参数用来控制是否使用 HMM 模型。</p>
<p><code>jieba.cut_for_search</code> 方法接受两个参数：需要分词的字符串；是否使用 HMM 模型。该方法适合用于搜索引擎构建倒排索引的分词，粒度比较细。</p>
<h3 id="hanlp">HanLP 分词</h3>
<h4 id="pyhanlp">pyhanlp 安装</h4>
<p>其为 HanLP 的 Python 接口，支持自动下载与升级 HanLP，兼容 Python2、Python3。</p>
<p>安装命令为 <code>pip install pyhanlp</code>，使用命令 hanlp 来验证安装。</p>
<p>pyhanlp 目前使用 jpype1 这个 Python 包来调用 HanLP，如果遇到：</p>
<blockquote>
  <p>building '_jpype' extensionerror: Microsoft Visual C++ 14.0 is required. Get it with "Microsoft VisualC++ Build Tools": http://landinghub.visualstudio.com/visual-cpp-build-tools</p>
</blockquote>
<p><strong>则推荐利用轻量级的 Miniconda 来下载编译好的 jpype1。</strong></p>
<pre><code>    conda install -c conda-forge jpype1
    pip install pyhanlp
</code></pre>
<p><strong>未安装 Java 时会报错</strong>：</p>
<blockquote>
  <p>jpype._jvmfinder.JVMNotFoundException: No JVM shared library file (jvm.dll) found. Try setting up the JAVA_HOME environment variable properly.</p>
</blockquote>
<p>HanLP 主项目采用 Java 开发，所以需要 Java 运行环境，请安装 JDK。</p>
<h4 id="-1">命令行交互式分词模式</h4>
<p>在命令行界面，使用命令 hanlp segment 进入交互分词模式，输入一个句子并回车，HanLP 会输出分词结果：</p>
<p><img src="http://images.gitbook.cn/009c2f60-616f-11e8-b864-0bd1f4b74dfb" alt="enter image description here" /></p>
<p>可见，pyhanlp 分词结果是带有词性的。</p>
<h4 id="-2">服务器模式</h4>
<p>通过 hanlp serve 来启动内置的 HTTP 服务器，默认本地访问地址为：http://localhost:8765 。</p>
<p><img src="http://images.gitbook.cn/d29d52f0-616f-11e8-b864-0bd1f4b74dfb" alt="enter image description here" /></p>
<p><img src="http://images.gitbook.cn/e79a06b0-6171-11e8-b864-0bd1f4b74dfb" alt="enter image description here" /></p>
<p>也可以访问官网演示页面：<a href="http://hanlp.hankcs.com/">http://hanlp.hankcs.com/</a>。</p>
<h4 id="hanlp-1">通过工具类 HanLP 调用常用接口</h4>
<p>通过工具类 HanLP 调用常用接口，这种方式应该是我们在项目中最常用的方式。</p>
<p>（1）分词</p>
<pre><code>    from pyhanlp import *
    content = "现如今，机器学习和深度学习带动人工智能飞速的发展，并在图片处理、语音识别领域取得巨大成功。"
    print(HanLP.segment(content))
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>[现如今/t, ，/w, 机器学习/gi, 和/cc, 深度/n, 学习/v, 带动/v, 人工智能/n, 飞速/d, 的/ude1, 发展/vn, ，/w, 并/cc, 在/p, 图片/n, 处理/vn, 、/w, 语音/n, 识别/vn, 领域/n, 取得/v, 巨大/a, 成功/a, 。/w]</p>
</blockquote>
<p>（2）自定义词典分词</p>
<p>在没有使用自定义字典时的分词。</p>
<pre><code>    txt = "铁甲网是中国最大的工程机械交易平台。"
    print(HanLP.segment(txt))
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>[铁甲/n, 网/n, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程/n, 机械/n, 交易/vn, 平台/n, 。/w]</p>
</blockquote>
<p>添加自定义新词：</p>
<pre><code>    CustomDictionary.add("铁甲网")
    CustomDictionary.insert("工程机械", "nz 1024")
    CustomDictionary.add("交易平台", "nz 1024 n 1")
    print(HanLP.segment(txt))
</code></pre>
<p>结果为：</p>
<blockquote>
  <p>[铁甲网/nz, 是/vshi, 中国/ns, 最大/gm, 的/ude1, 工程机械/nz, 交易平台/nz, 。/w]</p>
</blockquote>
<p>当然了，jieba 和 pyhanlp 能做的事还有很多，关键词提取、自动摘要、依存句法分析、情感分析等，后面章节我们将会讲到，这里不再赘述。</p>
<p>参考文献：</p>
<ol>
<li>https://github.com/fxsjy/jieba</li>
<li>https://github.com/hankcs/pyhanlp</li>
</ol>
<blockquote>
  <p><a href="https://gitbook.cn/m/mazi/comp/column?columnId=5b10b073aafe4e5a7516708b&utm_source=syjsd001">点击了解更多《中文自然语言处理入门》</a></p>
</blockquote></div></article>