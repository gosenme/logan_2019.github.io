---
title: 机器学习极简入门-29
---
<article id="topicContainer" class="column_content"><h2 class="topic_title"></h2><div><h3 id="">概率无向图</h3>
<h4 id="-1">定义</h4>
<p><strong>概率无向图模型</strong>（Probabilistic Undirected Graphical Model）是一个可以用无向图表示的联合概率分布。</p>
<p>它的整体结构是一张图（Graph），图中每一个节点表示一个或者一组变量，节点之间的边表示这两个/组变量之间的依赖关系。</p>
<p>概率无向图模型还有一个名字——<strong>马尔可夫随机场</strong>。</p>
<p>下图就是一个简单的马尔可夫随机场：</p>
<p><img src="https://images.gitbook.cn/04e8bb20-8eeb-11e8-b4db-8317426c96d5" alt="enter image description here" /></p>
<h4 id="-2">势函数和团</h4>
<p>关于马尔可夫随机场，有几个非常重要的概念。</p>
<p><strong>势函数</strong>（Potential Function，又称为因子 Factor）：是定义在变量子集上的非负实函数，用于定义概率分布函数。</p>
<p><strong>团</strong>（Clique）：图中节点的子集，其中任意两个节点之间都有边连接。</p>
<p><strong>极大团</strong>：一个团，其中加入任何一个其他的节点都不能再形成团。</p>
<p>马尔可夫随机场中，<strong>多个变量之间的联合概率分布可以基于团分解为多个势函数的乘积，每个势函数仅与一个团相关</strong>。</p>
<h4 id="hammersleyclifford">Hammersley-Clifford 定理</h4>
<p>对于 $N$ 个变量的马尔可夫随机场，其变量为 $X=(X_1,X_2, …X_N) $（上图例子中 $N=6$）。</p>
<p>设所有团构成的集合为 $C$，与团 $Q \in C$ 对应的变量集合记作 $X_Q$，则联合概率为：</p>
<p>$P(X) = \frac{1}{Z}\prod_{Q \in C} \Psi_Q(X_Q)$</p>
<p>其中 $\Psi_Q$ 为与团 $Q$ 对应的势函数，用于对团 $Q$ 中的变量关系进行建模。</p>
<p>$Z$ 为规范化因子，很多时候要计算它很困难，不过好在大多数情况下，我们无须计算出 $Z$ 的精确值。</p>
<p>当团 $Q$ 不是极大团的时候，它必然属于某个极大团——实际上每一个非极大团都是如此，此时我们完全可以只用极大团来计算 $P(X)$：$P(X) = \frac{1}{Z^*} \prod_{Q \in C^*} \Psi_Q(X_Q)$，其中 $C^*$ 为所有极大团的集合。</p>
<p>这叫做 Hammersley-Clifford 定理，是随机场（Random Fields）的基础定理，它给出了一个马尔可夫随机场被表达为<strong>正概率分布</strong>的<strong>充分必要条件</strong>。</p>
<h4 id="-3">性质</h4>
<p>在讨论马尔可夫随机场的性质之前，我们要学习两个概念。</p>
<h5 id="-4"><strong>两个概念</strong></h5>
<p>(1)分离（Separating）</p>
<p>设 $A、B、C$ 都是马尔可夫随机场中的节点集合，若从 $A$ 中的节点到 $B$ 中的节点都必须经过 $C$ 中的节点，则称 $A$ 和 $B$ 被 $C$ 分离，$C$ 称为<strong>分离集</strong>（Separating Set）。参加下图：</p>
<p><img src="https://images.gitbook.cn/54ae2fb0-80f3-11e8-a935-d59fe50595b6" alt="enter image description here" /></p>
<p>(2)马尔可夫性（Markov Property）</p>
<p>马尔可夫性的<strong>原始定义</strong>为：当一个随机过程在给定当前状态及所有过去状态情况下，其未来状态的条件概率分布仅依赖于当前状态；换句话说，在给定当前状态时，它与过去状态（即该过程的历史路径）是条件独立的，那么此随机过程即具有<strong>马尔可夫性</strong>。</p>
<p>我们把马尔可夫性引入到马尔可夫随机场中，把当前状态看作无向图中的一个节点，过去状态看作与当前状态节点有历史路径（边）连接的其他节点。</p>
<p>可以这样理解：在马尔可夫随机场的无向图中，任何一个节点的概率分布都仅和与它相连的节点有关。</p>
<p>形式化的表达为：设 $v$ 为无向图中任意一个节点，$W$ 是所有与 $v$ 相连的节点的集合，则 $v$ 的概率分布仅和 $W$ 有关，和 $v$ 与 $W$ 之外的其他节点无关 。</p>
<p>参看下图，给定灰色节点，则黑色节点独立于其他所有节点：</p>
<p><img src="https://images.gitbook.cn/c38936a0-8f08-11e8-90c0-476a2623d2fb" alt="enter image description here" /></p>
<h5 id="-5"><strong>马尔可夫随机场的马尔可夫性</strong></h5>
<p>马尔可夫随机场具备<strong>全局马尔可夫性</strong>（Global Markov Property）：给定两个变量子集的分离集，则这两个变量子集条件独立。</p>
<p>令 $A、B$ 和 $C$ 对应的变量集合分别为 $X_A、X_B、X_C$，则 $X_A$ 和 $X_B$ 在给定 $X_C$ 的条件下独立，记作：$X_A \perp X_B | X_C$。</p>
<p>即：$P(X_A, X_B|X_C) = P(X_A|X_C)P(X_B|X_C)$</p>
<p>由全局马尔可夫性，又可以推导出两种性质：</p>
<ul>
<li><strong>局部马尔可夫性</strong>（Local Markov Property）：给定某变量的邻接变量，则该变量条件独立于其他变量。</li>
</ul>
<p>用公式描述是：$P(X_v, X_O | X_W) = P(X_v|X_W)P(X_O|X_W)$，其中 $v$ 为无向图中任意节点，$W$ 是与 $v$ 有边连接的所有节点的集合，$O$ 是 $v$ 和 $W$ 之外的所有其他节点。</p>
<ul>
<li><strong>成对马尔可夫性</strong>（Pairwise Markov Property）：给定所有其他变量，两个非连接变量条件独立。</li>
</ul>
<p>公式描述为：$P(X_u, X_v | X_O) = P(X_u|X_O)P(X_v|X_O)$， 其中 $u$ 和 $v$ 为无向图中任意两个没有边连接的点，$O$ 为其他所有点的集合。</p>
<h3 id="conditionalrandomfieldcrf">条件随机场（Conditional Random Field，CRF）</h3>
<h4 id="-6">无向图模型</h4>
<p>CRF 也是一种无向图模型。</p>
<p>它<strong>和马尔可夫随机场的不同点在于：马尔可夫随机场是生成式模型</strong>，直接对联合分布进行建模；而<strong>条件随机场是判别式模型</strong>，对条件分布进行建模。</p>
<p>但两者又是相关的，CRF 是“有条件的”马尔可夫随机场。也就是说，CRF 是<strong>给定随机变量 $X$ 条件下，随机变量 $Y$ 的马尔可夫随机场</strong>。</p>
<h4 id="-7">定义</h4>
<p>这里我们给出 CRF 的<strong>定义</strong>：设 $X$ 和 $Y$ 是随机变量，$P(Y|X)$ 是给定 $X$ 条件下 $Y$ 的条件概率分布。如果随机变量 $Y$ 构成一个由无向图 $G=&lt;V, E&gt;$ 表示的马尔可夫随机场，则称条件概率分布 $P(Y|X)$ 为 CRF。</p>
<p>换言之，设 $X$ 和 $Y$ 是随机变量，$P(Y|X)$ 是给定 $X$ 条件下 $Y$ 的条件概率分布。如果随机变量 $Y$ 构成一个无向图 $G=&lt;V，E&gt;$，且图 $G$ 中每一个变量 $Y_v$ 都满足马尔可夫性——$P(Y_v|X, Y_Z) = P(Y_v|X, Y_W) $，其中 $Z$ 表示无向图中节点 $v$ 以外所有点的集合，$W$ 表示无向图中与节点 $v$ 有边连接的所有节点集合——则 $P(Y|X)$ 为 CRF。</p>
<p>在 $P(Y|X)$ 中，$X$ 是输入变量，表示需要标注的观测序列；$Y$ 是输出变量，表示状态（或称标记）序列。</p>
<p>从定义层面，没有要求 $X$ 和 $Y$ 具有相同结构，不过在实际运行中，一般假设 $X$ 和 $Y$ 具备相同图结构。</p>
<h3 id="crf">线性链 CRF</h3>
<p>在现实应用中，最常被用到的 CRF 是线性链（Linear Chain）CRF，其结构如下：</p>
<p><img src="https://images.gitbook.cn/67a4c520-80f3-11e8-876c-25ed94be3d09" alt="enter image description here" /> </p>
<p>当 $X$ 和 $Y$ 具备相同结构时，其形如下：</p>
<div style="text-align:center">
    <img src="https://images.gitbook.cn/7b5eaa90-80f3-11e8-a935-d59fe50595b6" width="500px" />
</div>
<p></br></p>
<p>上图中，$X$ 为观测序列，$Y$ 为状态序列。</p>
<p>设 $X=(X_1,X_2, …, X_n)$，$Y = (Y_1, Y_2, …, Y_n)$，它们都是线性链表示的随机变量序列。</p>
<p>如果在给定随机变量序列 $X$ 的条件下，随机变量序列 $Y$ 的条件概率分布 $P(Y|X)$ 构成 CRF，也就是说它满足马尔可夫性：</p>
<p>$P(Y_i|X, Y_1, …, Y_{i-1}, Y_{i+1}, …, Y_n) = P(Y_i|X, Y_{i-1}, Y_{i+1})$</p>
<p>其中 $i=1,2,…, n$ （当 $i=1$ 和 $n$ 时只考虑单边），则称 $P(Y|X)$ 为线性链条件随机场。$X$ 为输入序列/观测序列，$Y$ 为输出序列/标记序列/状态序列。</p>
<h3 id="hmmvscrf">HMM VS 线性链 CRF</h3>
<p>看到此处是不是很眼熟，是不是又想到了 HMM？</p>
<p>确实，HMM 和 CRF 看起来蛮像的。</p>
<p><img src="https://images.gitbook.cn/8bff1b00-80f3-11e8-9614-476580d74a06" alt="enter image description here" /></p>
<p>但是要注意：</p>
<ol>
<li>HMM 是有向图，CRF 是无向图；</li>
<li>HMM 计算的是状态和观测的联合概率，而 CRF 计算的是状态基于观测的条件概率。</li>
</ol>
<p>从使用的角度，HMM 多用于那种状态“原生”，而观测是状态“生成”出来的场景。</p>
<p>比如，用 HMM 来生成一段语音，则状态对应的是音节（声韵母）或文字，而观测则是这个音节所对应的声学特征。</p>
<p>这时，状态是相对客观的，观测是状态的一种“表征”，是状态“产生”出来的——我们想象一下自己说话时的场景，也是头脑中先想好说什么话，有了语言文字音节，然后再由大脑指挥喉舌发声。发出来的声音，就是最终的观测。</p>
<p>CRF 则多用于那种观测“原生”。状态“后天”产生，用来标记观测的情况。</p>
<p>比如，用 CRF 来做文本实体标记。输入一句话“我有一个苹果”，CRF 处理后将“苹果”标记成了“水果”。这个时候，“苹果”是观测，而“水果”则是对应的状态（或称标签）。</p>
<p>同一个观测值“苹果”，它的标签可以是“水果”，也可以是“手机”，具体是什么与训练数据有关，也与之前状态值有关。</p>
<p>但无论怎么样，观测才是客观存在的。而状态（标签）是人为“打”上去的，是以观测为条件进行“判别”的结果。</p></div></article>