---
title: 机器学习极简入门-24
---
<article id="topicContainer" class="column_content"><h2 class="topic_title"></h2><div><h3 id="">严格的线性回归</h3>
<p>之前我们讲过<strong>线性回归</strong>：在向量空间里用线性函数去拟合样本。</p>
<p>该模型以所有样本实际位置到该线性函数的综合距离为损失，通过最小化损失来求取线性函数的参数。参见下图：</p>
<p><img src="http://images.gitbook.cn/2bcea220-7f7a-11e8-8f4f-398ed9d7e1c5" alt="enter image description here" /> </p>
<p>对于线性回归而言，一个样本只要不是正好落在最终作为模型的线性函数上，就要被计算损失。</p>
<p>如此严格，真的有利于得出可扩展性良好的模型吗？</p>
<h3 id="svr">宽容的支持向量回归（SVR）</h3>
<p>今天我们来介绍一种“宽容的”回归模型：<strong>支持向量回归（Support Vector Regression，SVR）</strong>。</p>
<h4 id="-1">模型函数</h4>
<p>支持向量回归模型的<strong>模型函数</strong>也是一个线性函数： y = wx + b。</p>
<p>看起来和线性回归的模型函数一样！</p>
<p>但 <strong>SVR 和线性回归</strong>，却是<strong>两个不同的回归模型</strong>。</p>
<p>不同在哪儿呢？不同在学习过程。</p>
<p>说得更详细点，就是：<strong>计算损失的原则不同，目标函数和最优化算法也不同</strong>。</p>
<h4 id="-2">原理</h4>
<p><strong>SVR</strong> 在线性函数两侧制造了一个“间隔带”，对于所有落入到间隔带内的样本，都不计算损失；只有间隔带之外的，才计入损失函数。之后再通过最小化间隔带的宽度与总损失来最优化模型。</p>
<p>如下图这样，只有那些圈了红圈的样本（或在隔离带边缘之外，或落在隔离带边缘上），才被计入最后的损失：</p>
<p><img src="http://images.gitbook.cn/3ffca620-7f7a-11e8-8f4f-398ed9d7e1c5" alt="enter image description here" /> </p>
<h4 id="svr-1">SVR 的两个松弛变量</h4>
<p>这样看起来，是不是 SVR 很像 SVM？</p>
<p>不过请注意，有一点 SVR 和 SVM <strong>正相反</strong>，那就是：SVR 巴不得所有的样本点都落在“隔离带”里面，而 SVM 则恰恰希望所有的样本点都在“隔离带”之外！</p>
<p>正是这一点区别，导致 SVR 要同时引入两个而不是一个松弛变量。</p>
<p><strong>SVR 引入两个松弛变量：$\xi$ 和 $\xi^*$</strong></p>
<p><img src="https://images.gitbook.cn/4e1fc450-9ea4-11e8-8324-45c28b509596" alt="enter image description here" /></p>
<p>上图显示了 SVR 的基本情况：</p>
<p>$f(x) = wx +b$ 是我们最终要求得的模型函数；</p>
<p>$wx + b + \epsilon$ 和 $wx +b – \epsilon$（也就是 $f(x) + \epsilon$ 和 $f(x) - \epsilon$）是隔离带的上下边缘；</p>
<p>$\xi$ 是隔离带上边缘之上样本点的 $y$ 值，与对应 $x$ 坐标在“上边缘超平面”上投影的差；</p>
<p>而 $\xi^*$ 则是隔离带下边缘之下样本点，到隔离带下边缘上的投影，与该样本点 $y$ 值的差。</p>
<p>这样说有些绕，我们用公式来表达：</p>
<p><img src="https://images.gitbook.cn/e62cefa0-a671-11e8-a6df-e5b5930cc16b" alt="" /></p>
<p>对于任意样本 $x_i$，如果它在隔离带里面或者隔离带边缘上，则 $\xi_i$ 和 $\xi_i^*$ 都为$0$； 如果它在隔离带上边缘上方，则 $\xi_i &gt; 0，\xi_i^*=0$；如果它在下边缘下方，则 $\xi_i = 0，\xi_i^* &gt; 0$。</p>
<h3 id="svr-2">SVR 的主问题和对偶问题</h3>
<h4 id="svr-3">SVR 的主问题</h4>
<p>SVR 主问题的数学描述如下：</p>
<p>$min_{w,b,\xi,\xi^*}\frac{1}{2}||w||^2 + C\sum_{i=1}^{m}(\xi_i +\xi_i^*)$</p>
<p>$ s.t. \,\, f(x_i) - y_i \leqslant \epsilon + \xi_i;\;\; y_i - f(x_i)  \leqslant \epsilon + \xi_i^* ;\;\; \xi_i \geqslant 0; \;\; \xi_i^* \geqslant 0,\;\; i = 1,2, ..., m.$    </p>
<h4 id="svr-4">SVR 的拉格朗日函数和对偶问题</h4>
<p>我们引入拉格朗日乘子 $\mu_i \geqslant 0，\mu_i^* \geqslant 0， \alpha_i \geqslant 0$ 和 $\alpha_i^* \geqslant 0$，来针对上述主问题构建拉格朗日函数，得到<strong>拉格朗日函数</strong>如下：</p>
<p>$L(w,b,\xi,\xi^*, \alpha,\alpha^*, \mu,\mu^*) = \frac{1}{2}||w||^2 + C\sum_{i=1}^{m}(\xi_i +\xi_i^*)  + \sum_{i=1}^{m}\alpha_i(f(x_i) - y_i -\epsilon - \xi_i)  + \sum_{i=1}^{m}\alpha_i^*(y_i - f(x_i)  -\epsilon - \xi_i^*)  + \sum_{i=1}^{m}\mu_i(0 - \xi_i) + \sum_{i=1}^{m}\mu_i^*(0 - \xi_i^*) $    </p>
<p>它对应的<strong>对偶问题</strong>是：</p>
<p>$max_{\alpha, \alpha^*, \mu, \mu^*}min_{w,b,\xi,\xi^*}L(w,b,\xi,\xi^*, \alpha,\alpha^*, \mu,\mu^*) $</p>
<h4 id="svr-5">求解 SVR 对偶问题</h4>
<p>按照我们前面学习过的方法，首先要求最小化部分：</p>
<p>$min_{w,b,\xi,\xi^*}L(w,b,\xi,\xi^*, \alpha,\alpha^*, \mu,\mu^*) $</p>
<p>分别对 $w、b、\xi_i 和 \xi_i^*$ 求偏导，并令偏导为0，可得：</p>
<p>$ w = \sum_{i=1}^{m}(\alpha_i^* - \alpha_i)x_i, $</p>
<p>$ 0 = \sum_{i=1}^{m}(\alpha_i^* - \alpha_i),$</p>
<p>$ C = \alpha_i + \mu_i ,$</p>
<p>$ C = \alpha_i^* + \mu_i^* .$</p>
<p>将上述4个等式带回到对偶问题中，在通过求负将极大化问题转化为极小化问题，得到如下结果：</p>
<p>$\\ min_{\alpha,\alpha^*}[\sum_{i=1}^{m}y_i(\alpha_i - \alpha_i^*) + \epsilon\sum_{i=1}^{m}(\alpha_i + \alpha_i^*)+ \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}(\alpha_i -\alpha_i^*)(\alpha_j -\alpha_j^*)x_i^Tx_j ]$        </p>
<p>$\\ s.t. \sum_{i=1}^{m}(\alpha_i^* - \alpha_i) = 0; \;\; 0 \leqslant \alpha_i, \alpha_i^* \leqslant C. $</p>
<h4 id="smosvr">用 SMO 算法求解 SVR</h4>
<p>到了这一步我们可以采用 SMO 算法了吗？</p>
<p>直觉上，好像不行。因为 SMO 算法针对的是任意样本 $x_i$ 只对应一个参数 $\alpha_i$ 的情况，而此处，这个样本却对应两个参数 $\alpha_i 和 \alpha_i^*$。</p>
<p>有没有办法把 $\alpha_i$ 和 $\alpha_i^*$ 转化为一个参数呢？办法还是有的！</p>
<p>我们整个求解过程采用的是拉格朗日对偶法，对偶问题有解的<strong>充要条件</strong>是满足 KKT 条件。</p>
<p>那么对于 SVR 的对偶问题，它的 KKT 条件是什么呢？它的 <strong>KKT 条件</strong>如下：</p>
<p>$ \alpha_i(f(x_i) - y_i - \epsilon - \xi_i) = 0, $    </p>
<p>$ \alpha_i^*(y_i - f(x_i) - \epsilon - \xi_i^* ) = 0, $</p>
<p>$ \alpha_i\alpha_i^*=0, $</p>
<p>$\xi_i\xi_i^* = 0, $</p>
<p>$(C - \alpha_i)\xi_i = 0, $</p>
<p>$ (C - \alpha_i^*)\xi_i^* = 0. $</p>
<p>由 KKT 条件可见，当且仅当 $f(x_i) - y_i - \epsilon - \xi_i = 0$ 时，$\alpha_i$ 才可以取非0值；当且仅当 $y_i - f(x_i) - \epsilon - \xi_i^* = 0$ 时，$\alpha_i^*$ 才可以取非0值。</p>
<p>$f(x_i) - y_i - \epsilon - \xi_i = 0 =&gt; y_i = f(x_i) - \epsilon - \xi_i$ 对应的是在隔离带下边缘以下的样本。</p>
<p>而 $y_i - f(x_i) - \epsilon - \xi_i^* = 0 =&gt; y_i =  f(x_i) + \epsilon + \xi_i^*$ 对应的是在隔离带上边缘之上的样本。</p>
<p>一个样本不可能同时既在上边缘之上，又在上边缘之下，所以这两个等式最多只有一个成立，相应的 $\alpha_i$ 和 $\alpha_i^*$ 中至少有一个为0。</p>
<p>我们设：$\lambda_i = \alpha_i – \alpha_i^*$。</p>
<p>既然 $\alpha_i$ 和 $\alpha_i^*$ 中至少有一个为0，且 $0 \leqslant \alpha_i，\alpha_i^*，\leqslant C $ （参见上节“求解 SVR 对偶问题”结尾），于是有 : $|\lambda_i| = \alpha_i + \alpha_i^*$。</p>
<p>将 $\lambda_i$ 和 $|\lambda_i|$ 带入对偶问题，则有：</p>
<p>$\\ min_{\lambda}[\sum_{i=1}^{m}y_i(\lambda_i) + \epsilon\Sigma_{i=1}^{m}( |\lambda_i|) + \frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\lambda_i \lambda_j x_i^Tx_j ]$</p>
<p>$\\ s.t. \sum_{i=1}^{m}(\lambda_i) = 0; \;\;-C \leqslant \lambda_i \leqslant C. $</p>
<p>如此一来，不就可以应用 SMO 求解了嘛！</p>
<p>当然，这样一个推导过程仅仅用于说明 SMO 也可以应用于 SVR，具体的求解过程和 SVM 的 SMO 算法还是有所差异的。</p>
<h3 id="-3">支持向量与求解线性模型参数</h3>
<p>因为 $f(x) = wx + b$，又因为前面已经求出 $ w = \sum_{i=1}^{m}(\alpha_i^* - \alpha_i)x_i $，因此：</p>
<p>$f(x) = \sum_{i=1}^{m}(\alpha_i^* - \alpha_i)x_i^Tx + b$</p>
<p>由此可见，只有满足 $\alpha_i^* - \alpha_i  \ne 0$ 的样本才对 $w$ 取值有意义，才是 SVR 的支持向量。</p>
<p>也就是说，只有当样本满足下列两个条件之一时，它才是支持向量：</p>
<p>$f(x_i) - y_i - \epsilon - \xi_i = 0$</p>
<p>或</p>
<p>$ y_i - f(x_i) - \epsilon - \xi_i^* = 0$</p>
<p>换言之，这个样本要么在隔离带上边缘以上，要么在隔离带下边缘以下（含两个边缘本身）。</p>
<p>也就是说，落在 $\epsilon$-隔离带之外的样本，才是 SVR 的支持向量！</p>
<p>可见，无论是 SVM 还是 SVR，它们的解都仅限于支持向量，即只是全部训练样本的一部分。因此 SVM 和 SVR 的解都具有稀疏性。</p>
<p>通过最优化方法求解出了 $w$ 之后，我们还需要求 $b$。</p>
<p>$f(x_i) = wx_i + b =&gt; b = f(x_i) – wx_i $</p>
<p>而且，对于那些落在隔离带上边缘上的支持向量，有 $f(x_i)= y_i+\epsilon$，落在隔离带下边缘上的支持变量有 $f(x_i) = y_i -\epsilon$。</p>
<p>因此，$ b = \frac{1}{|S_u| + |S_d|}[\sum_{s\in S_u}(y_s + \epsilon - w  x_s) + \sum_{s\in S_d}(y_s - \epsilon - w  x_s)]$    </p>
<p>其中 $S_u$ 是位于隔离带上边缘的支持向量集合，而 $S_d$ 则是位于隔离带下边缘的支持向量集合。</p>
<h3 id="svr-6">SVR 的核技巧</h3>
<p>前面讲到的适用于 SVM 的核技巧也同样适用于 SVR。</p>
<p>SVR 核技巧的实施办法和 SVM 一样，也是将输入空间的 $x$ 通过映射函数 $\phi(x)$ 映射到更高维度的特征空间。然后再在特征空间内做本文前面所述的一系列操作。</p>
<p>因此，在特征空间中的线性模型为：</p>
<p>$f(x) = w\phi(x) + b$</p>
<p>其中：</p>
<p>$ w = \sum_{i=1}^{m}(\alpha_i^* - \alpha_i)\phi(x_i) $</p>
<p>对照 SVM 核函数的做法，我们也令：</p>
<p>$k(x_i,x_j) = \phi(x_i)^T \phi(x_j)$</p>
<p>则： </p>
<p>$f(x) = \sum_{i=1}^{m}(\alpha_i^* - \alpha_i)k(x,x_i) + b$</p>
<p>具体核技巧的实施过程，也对照 SVM 即可。</p></div></article>