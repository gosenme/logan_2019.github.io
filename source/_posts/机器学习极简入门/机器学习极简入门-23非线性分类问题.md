---
title: 机器学习极简入门-23
---
<article id="topicContainer" class="column_content"><h2 class="topic_title"></h2><div><h3 id="">非线性分类问题</h3>
<p>遇到分类问题的时候，最理想的状态，当然是样本在向量空间中都是线性可分的，我们可以清晰无误地把它们分隔成不同的类别——<strong>线性可分 SVM</strong>。</p>
<p>如果实在不行，我们可以容忍少数不能被正确划分，只要大多数线性可分就好——<strong>线性 SVM</strong>。</p>
<p>可是，如果我们面对的分类问题，根本就是<strong>非线性</strong>的呢？比如像下面这样：</p>
<div style="text-align:center">
    <img src="http://images.gitbook.cn/2b41d0a0-7f68-11e8-8f4f-398ed9d7e1c5" width="400px" />
</div>
<p></br></p>
<p>图中红色的点是正类样本，蓝色的点是负类样本。通过我们的观察可知，它们之间的界限是很分明的，用图中绿色的圈本来可以把它们完全分开。</p>
<p>很可惜，“圆圈”在二维空间里无法用线性函数表示，也就是说这些样本在二维空间里根本线性不可分。所以，无论是线性可分 SVM 还是线性 SVM，都无法在这些样本上良好工作。</p>
<p>这可怎么办呢？难道，这种情况我们就处理不了了？</p>
<p>并不是！</p>
<p>我们可以想个办法，让这些在二维空间中线性不可分的样本，<strong>在更高维度的空间里线性可分</strong>。</p>
<p>比如说，如果我们能把上图中那些正负类的样本映射到三维空间中，并且依据不同的类别给它们赋予不一样的高度值——$z$ 轴取值（就像下图这样），那么不就线性可分了嘛。</p>
<p><img src="http://images.gitbook.cn/34166790-7f68-11e8-a88c-dd6ae79624fa" alt="enter image description here" /></p>
<p>如此一来，在二维空间团团转的正负例，在三维空间中分为两层，中间用一个超平面，就可以完美分隔了。</p>
<h3 id="svm">非线性 SVM</h3>
<h4 id="svm-1">非线性 SVM 分隔超平面</h4>
<p>对于在有限维度向量空间中线性不可分的样本，我们将其映射到更高维度的向量空间里，再通过间隔最大化的方式，学习得到支持向量机，就是<strong>非线性 SVM</strong>。</p>
<p>我们将样本映射到的这个更高维度的新空间叫做<strong>特征空间</strong>。</p>
<blockquote>
  <p>注意：如果是理想状态，样本从原始空间映射到特征空间后直接就成为线性可分的，那么接下来的学习是可以通过硬间隔最大化的方式来学的。</p>
  <p>不过，一般的情况总没有那么理想，因此，通常情况下，我们还是按照软间隔最大化，在特征空间中学习 SVM。</p>
</blockquote>
<p>简单理解就是：<strong>非线性 SVM = 核技巧 + 线性 SVM</strong>。</p>
<p>我们用向量 $x$ 表示位于原始空间中的样本，$\phi(x)$ 表示 $x$ 映射到特征空间之后的新向量。</p>
<p>则<strong>非线性 SVM</strong>对应的<strong>分隔超平面为：$f(x) = w\phi(x) + b$</strong>。</p>
<h4 id="svm-2">非线性 SVM 的对偶问题</h4>
<p>套用上一篇线性 SVM 的对偶问题，此处<strong>非线性 SVM 的对偶问题</strong>就变成了：</p>
<p>$min(\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j\phi(x_i)\cdot \phi(x_j)-\sum_{i=1}^{m}\alpha_i)$</p>
<p>$ s.t. \,\,\,\, \sum_{i=1}^{m}\alpha_iy_i = 0$</p>
<p>$ 0 \leqslant \alpha_i \leqslant C ,\,\,\,\,  i = 1,2, ..., m$</p>
<p>大家可以看到，<strong>和线性 SVM 唯一的不同</strong>就是：之前的 <strong>$x_i$ 与 $x_j$ 的内积（点乘） 变成了 $\phi(x_i)$ 与 $\phi(x_j)$ 的内积</strong>。</p>
<h4 id="-1">核函数</h4>
<p>对于有限维的原始空间，一定存在更高维度的空间，使得前者中的样本映射到新空间后可分。但是新空间（特征空间）的维度也许很大，甚至可能是无限维的。这样的话，直接计算 $\phi(x_i)·\phi(x_j)$ 就会很困难。</p>
<p>为了避免计算 $\phi(x_i)$ 和 $\phi(x_j)$ 的内积，我们需要设置一个新的函数——$ k(x_i, x_j)$：</p>
<p>$ k(x_i, x_j) =\phi(x_i) \cdot \phi(x_j) $</p>
<p>原始空间中的两个样本 $x_i$ 和 $x_j$ 经过 $k(·,·)$ 函数计算所得出的结果，是它们在特征空间中映射成的新向量的内积。</p>
<p>如此一来，我们就不必真的计算出 $\phi(x_i)$ 点乘 $\phi(x_j)$ 的结果，而可以直接用 $k(·,·)$ 函数代替它们。</p>
<p>我们把这个 $k(·,·)$ 函数叫做核函数。现在我们给出它的正式定义：</p>
<p>设 $ \mathcal{X}$ 为原始空间（又称输入空间），$\mathcal{H}$ 为特征空间（特征空间是一个带有内积的完备向量空间，又称完备内积空间或希尔伯特空间）。</p>
<p>如果存在一个映射： $ \mathcal{X} \times \mathcal{X}$    ，使得对所有 $ x_i, x_j \in \mathcal{X} $，函数 $ k(x_i, x_j)$ 满足条件：$ k(x_i, x_j) =\phi(x_i) \cdot \phi(x_j) $，即 $k(·,·)$ 函数为输入空间中任意两个向量映射到特征空间后的内积。则称 $k(·,·)$ 为核函数，$\phi(·)$ 为映射函数。</p>
<h4 id="svm-3">运用核技巧求解非线性 SVM 的对偶问题</h4>
<p>有了核函数，我们就可以将非线性 SVM 的对偶问题写成：</p>
<p>$min(\frac{1}{2}\sum_{i=1}^{m}\sum_{j=1}^{m}\alpha_i\alpha_jy_iy_j k(x_i, x_j) - \sum_{i=1}^{m}\alpha_i  )$</p>
<p>$ s.t. \,\,\,\, \sum_{i=1}^{m}\alpha_iy_i = 0$</p>
<p>$ 0 \leqslant \alpha_i \leqslant C ,\,\,\,\,  i = 1,2, ..., m$</p>
<p>之后的求解过程与线性 SVM 一致：先根据对偶问题求解 $\alpha$，再根据 $\alpha$ 的结果计算 $w$，然后根据支持向量求解 $b$，在此就不赘述了。</p>
<p>相应地，最终求出的非线性 SVM 在特征空间的最大分隔超平面也就成了：</p>
<p>$ f(x) = w \phi(x) + b =  \sum_{i=1}^{m}\alpha_i y_i \phi(x_i)\cdot \phi(x) + b = \sum_{i=1}^{m}\alpha_i y_i k(x, x_i) + b$</p>
<p>上述运用核函数求解的过程，称为核技巧。</p>
<h3 id="-2">核函数的性质</h3>
<p>如果我们已经知道了映射函数是什么，当然可以通过两个向量映射后的内积直接求得核函数。</p>
<p>但是，在我们还不知道映射函数本身是什么的时候，有没有可能直接判断一个函数是不是核函数呢？</p>
<p>换句话说，<strong>一个函数需要具备怎样的性质，才是一个核函数</strong>？</p>
<p>以下是<strong>函数 $k(·,·)$ 可以作为核函数的充分必要条件</strong>：</p>
<p>设 $ \mathcal{X}$ 是输入空间，$k(·,·)$ 是定义在 $ \mathcal{X} \times \mathcal{X}$    上的对称函数，当且仅当任意 $ x_1, x_2, ..., x_m \in \mathcal{X} $， 核矩阵 $K$（如下所示）总是半正定时，$k(·,·)$ 就可以作为核函数使用。</p>
<p>核矩阵：</p>
<p><img src="https://images.gitbook.cn/8f3255f0-88bc-11e8-8555-ffb97409ea91" alt="" /></p>
<blockquote>
  <p>注意：下面是几个线性代数的基础概念。</p>
  <p>对称函数：函数的输出值不随输入变量的顺序改变而改变的函数叫做对称函数。</p>
  <p>因为输入空间中的 $x_i 和 x_j$ 是向量，特征空间中的 $\phi(x_i) 和 \phi(x_j)$ 也是向量，$k(x_i,x_j)$ 表示特征空间向量的内积，而两个向量的内积并不因为其顺序变化而变化。因此，$ k(x_i,x_j) = k（x_j,x_i)$，即 k$(·,·)$ 为对称函数。</p>
  <p>相应地，核矩阵 K 为对称矩阵。</p>
  <p>半正定矩阵：一个 $n \times n$ 的实对称矩阵 $M$ 为半正定，当且仅当对于所有非零实系数向量 $z$，均有： $ z^TMz \geqslant 0$。</p>
  <p>其中“非零实系数向量”的含义是：一个向量中所有元素均为实数，且其中至少有一个元素值非零。</p>
</blockquote>
<h3 id="-3">核函数的种类</h3>
<p>要知道，非线性 SVM 的<strong>关键</strong>在于将输入空间中线性不可分的样本映射到线性可分的特征空间中去。特征空间的好坏直接影响到了 SVM 的效果。</p>
<p><strong>如何选择核函数</strong>也就成了一个关键性的问题。</p>
<p>虽然我们已经学习了核函数的定义和性质，但让我们凭空去自己构建一个核函数出来，还是一个非常困难的任务。</p>
<p>即使真得构造出来了，这个核函数在具体问题上的效果如何，还需要通过大量测试来证明，很可能费了好大劲，最后效果还不理想。</p>
<p>好在前人已经给我们留下来一些经历了长久磨练的常用核函数，让我们可以直接拿来用。接下来，我们分别看下这些核函数。</p>
<h4 id="linearkernel">线性核（Linear Kernel）</h4>
<p>$k(x_i, x_j) = x_i^Tx_j$</p>
<p>使用时无须指定参数（Parameter），它直接计算两个输入向量的内积。经过线性核函数转换的样本，特征空间与输入空间重合，相当于并没有将样本映射到更高维度的空间里去。</p>
<p>很显然这是最简单的核函数，实际训练、使用 SVM 的时候，在不知道用什么核的情况下，可以先试试线性核的效果。</p>
<h4 id="polynomialkernel">多项式核（Polynomial Kernel）</h4>
<p>$k(x_i, x_j) = (\gamma x_i^Tx_j + r)^d, \,\,\,\, \gamma &gt; 0,\,\, d \geqslant 1$</p>
<p>需要指定三个参数：$\gamma$、$r$ 和 $d$。</p>
<p>这是一个不平稳的核，适用于数据做了归一化（Normalization，参见本文最后一节）的情况。</p>
<h4 id="rbfradialbasisfunctionkernel">RBF 核（Radial Basis Function Kernel）</h4>
<p>$k(x_i, x_j) = \exp(-\gamma||x_i - x_j||^2) , \,\,\,\, \gamma &gt; 0$</p>
<p>RBF 核又名高斯核 （Gaussian Kernel），是一个核函数家族。它会将输入空间的样本以非线性的方式映射到更高维度的空间（特征空间）里去，因此它可以处理类标签和样本属性之间是非线性关系的状况。</p>
<p>它有一个参数：$\gamma$，这个参数的设置非常关键！</p>
<p>如果设置过大，则整个 RBF 核会向线性核方向退化，向更高维度非线性投影的能力就会减弱；但如果设置过小，则会使得样本中噪音的影响加大，从而干扰最终 SVM 的有效性。</p>
<p>不过相对于多项式核的3个参数，RBF 核只有一个参数需要调，还是相对简单的。</p>
<p>当线性核效果不是很好时，可以用 RBF 试试。或者，很多情况下可以直接使用 RBF。</p>
<p>著名的 LIBSVM（台湾大学林智仁/Lin Chih-Jen 教授等设计开发的一款简单、易用、快速有效的 SVM/SVR 支持包）的默认核函数，就是 RBF 核。</p>
<h4 id="sigmoidsigmoidkernel">Sigmoid 核（Sigmoid Kernel）</h4>
<p>$k(x_i, x_j) = tanh(\gamma x_i^Tx_j + r)$</p>
<p>有两个参数，$\gamma$ 和 $r$，在某些参数设置之下，Sigmoid 核矩阵可能不是半正定的，此时 Sigmoid 核也就不是有效的核函数了。因此参数设置要非常小心。</p>
<p>整体而言，Sigmoid 核并不比线性核或者 RBF 核更好。但是，当参数设置适宜时，它会有不俗的表现。</p>
<p>在具体应用核函数时，最好针对具体问题参照前人的经验。</p>
<h3 id="-4">构建自己的核函数</h3>
<p>除了常见的核函数，我们还可以根据以下规律进行核函数的组合。</p>
<ol>
<li><p>与正数相乘：
$k(·,·)$ 是核函数，对于任意正数（标量）$\alpha \geqslant 0,  \;\; \alpha k(·,·)$ 也是核函数。</p></li>
<li><p>与正数相加：
$k(·,·)$ 是核函数，对于任意正数（标量）$\alpha \geqslant 0,  \;\; \alpha + k(·,·)$ 也是核函数。</p></li>
<li><p>线性组合：
$k(·,·)$ 是核函数，则如下线性组合也是核函数：$ \sum_{i=1}^{n}\alpha_i k_i(\cdot ,\cdot), \,\,\,\, \alpha_i \geqslant 0$。</p></li>
<li><p>乘积：
$k_1(·,·)$ 和 $k_2(·,·)$ 都是核函数，则它们的乘积——$k_1(·,·) k_2(·,·)$——也是核函数。</p></li>
<li><p>正系数多项式函数：
设 $P$ 为实数域内的正系数多项式函数，$k(·,·)$ 是核函数，则 $P(k(·,·))$ 也是核函数。</p></li>
<li><p>指数函数：
$k(·,·)$ 是核函数，则 $\exp{(k(·,·))}$ 也是核函数。</p></li>
</ol>
<p>掌握了这些规律，我们也可以尝试根据需要构建自己的核函数。</p>
<h3 id="datanormalization">数据归一化（Data Normalization）</h3>
<p>数据归一化是一种数据处理方法，具体所做的就是对取值范畴不同的数据进行归一化处理，使它们处在同一数量级上。</p>
<p>最常见的，就是把各种数据都变成 $(０,１)$ 之间的小数。</p>
<p><img src="http://images.gitbook.cn/8bb213f0-7f68-11e8-a167-a3b450d69a42" alt="enter image description here" /></p>
<p>上图是一个归一化过程在二维中的直观显示。</p>
<p>大家可以看到， “扁长”的数据分布，经过归一化处理之后，变成了一个“正圆”。</p>
<p>常用的归一化算法有：</p>
<p>(1)线性转换</p>
<p>$x' = \frac{(x - min)}{(max - min)}$</p>
<p>(2)标准分</p>
<p>$x'= \frac{(x – \mu)}{\gamma}$  </p>
<p>原本的 $x$ 符合正态分布，$\mu$ 为其分布的均值，而 $\gamma$ 为分布的方差。</p></div></article>