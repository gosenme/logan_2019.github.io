---
title: 机器学习极简入门-14
---
<article id="topicContainer" class="column_content"><h2 class="topic_title"></h2><div><h3 id="">不再简单地将频率当作概率</h3>
<p>已知朴素贝叶斯公式：</p>
<p>$P(C|F_1,F_2,…, F_n) =\frac{1}{Z}P(C) \prod_{i=1}^{n} P(F_i|C)$</p>
<p>其中，$F_i$ 表示样本的第 $i$ 个特征，$C$ 为类别标签。$P(F_i | C)$ 表示样本被判定为类别 $C$ 前提下，第 $i$ 个特征的条件概率。</p>
<p>之前，对于 $P(F_i |C)$ 我们用频率来作为概率的估计，就如同上面例子中做的那样。</p>
<p>现在我们要采用另外一种方式，<strong>通过该特征在数据样本中的分布来计算该特征的条件概率</strong>。</p>
<p>首先先明确一下符号：</p>
<ul>
<li>$D$：表示训练集；</li>
<li>$D_c$：表示训练集中最终分类结果为 $c$ 的那部分样本的集合；</li>
<li>$X$：表示一个训练样本（单个样本）；</li>
<li>$x_i^{(j)}$：表示第 $j$ 个样本的第 $i$ 个特征的特征值；</li>
<li>$m$：$D$ 中样本的个数；</li>
<li>$m_c$：$D_c$ 中样本的个数，一般情况下（$m_c &lt; m$）。</li>
</ul>
<p>我们假设：</p>
<ol>
<li><p><strong>$P(x_i | c)$ 具有特定的形式，这个具体的形式是事先就已经认定的，不需要求取。</strong></p></li>
<li><p><strong>$P(x_i | c)$ 被参数 $\theta_{c,i}$ 唯一确定。</strong></p></li>
</ol>
<p>那么，我们现在所拥有的是：</p>
<ol>
<li><strong>$P(x_i | c)$ 的形式。</strong></li>
<li><strong>数据集合 $D$，其中每个样本的第 $i$ 个特征都符合上述假定。</strong></li>
</ol>
<p>我们要做的是：<strong>利用 $D$ 求出 $\theta_{c,i}$ 的值</strong>。</p>
<p>这里举例说明一下，比如：$P(x_i |c)$ 符合高斯分布，则：</p>
<p>$P(x_i|c) = \frac{1}{{ \sqrt {2\pi \sigma_{c,i}^2 } }}exp( \frac{-(x_i - \mu_{c,i})^2 }{{2\sigma_{c,i} ^2 }}) $</p>
<p>我们知道，高斯分布又名正态分布（在二维空间内形成钟形曲线），每一个高斯分布由两个参数——均值和方差，即上式中的 $\mu_{c,i}$ 和 $\sigma_{c,i}$ ——决定，也就是说 $\theta_{c,i} = (\mu_{c,i} ，\sigma_{c,i} )$。</p>
<p><img src="http://images.gitbook.cn/fefe1740-4779-11e8-9e8f-c159b4fd4ecb" alt="enter image description here" /></p>
<p>如果我们认定特征 $x_i$ 具有高斯分布的形式，又知道了 $\mu_{c,i}$ 和 $\sigma_{c,i}$ 的取值，我们也就获得了对应 $x_i$ 的具体概率分布函数。</p>
<p>直接带入 $x_i$ 的值计算相应条件概率，再进一步计算所有特征条件概率的积，就可以得出当前样本的后验概率了。</p>
<p>问题就在于：我们不知道 $\theta_{c,i}$  （$\mu_{c,i}$ 和 $\sigma_{c,i}$）的值，因此首先需要求出它们。</p>
<p>$\theta_{c,i}$ 是我们要以 $D$ 为训练数据，通过训练过程得到的结果。</p>
<p>这个训练过程，要用到概率统计中<strong>参数估计（Parameter Estimation）</strong>的方法。</p>
<h3 id="-1">两个学派</h3>
<p><strong>注意：</strong> 这里插入一个有趣的小知识，如果不感兴趣，可以直接跳过本节！</p>
<p><img src="http://images.gitbook.cn/dfe88360-4790-11e8-9a2d-c1e768ea8f10" alt="enter image description here" /></p>
<p>统计学界有两个学派——<strong>频率学派（Frequentist）</strong>和<strong>贝叶斯学派（Bayesian）</strong>。这两个派系对于最基本的问题——世界的本质是什么样的——看法不同。</p>
<p><strong>频率学派</strong>认为：世界是确定的，有一个本体，这个本体的真值不变。我们的目标就是要找到这个真值或真值所在的范围。</p>
<p>具体到“求正态分布的参数值”的问题，他们认为：这两个参数虽然未知，但是在客观上存在固定值，我们要做的是通过某种准则，根据观察数据（训练数据）把这些参数值确定下来。</p>
<p><strong>贝叶斯学派</strong>认为：世界是不确定的，本体没有确定真值，而是其真值符合一个概率分布。我们的目标是找到最优的，可以用来描述本体的概率分布。</p>
<p>具体到“求正态分布的参数值”的问题，他们认为：这两个参数（均值和方差），本身也是变量，也符合某个分布。因此，可以假定参数服从一个先验分布，然后再基于观察数据（训练数据）来计算参数的后验分布。</p>
<p>本文我们讲的是朴素贝叶斯模型，那到了需要估计条件概率参数的时候，肯定是用贝叶斯学派的方法咯？还真<strong>不是</strong>！</p>
<p>这里，让我们来见识一下最常用的参数估计方法：<strong>频率学派的极大似然估计（Maximum Likelihood Estimation, MLE）</strong>。</p>
<p>具体讲解之前，我们先整理一下——</p>
<p>我们在学习的是<strong>朴素贝叶斯分类器</strong>，它是一个分类模型，它的模型函数是<strong>朴素贝叶斯公式</strong>——<strong>贝叶斯定理</strong>在所有特征全部独立情况下的特例。</p>
<p>贝叶斯定理的名称来自于18世纪的英国数学家<strong>托马斯 · 贝叶斯</strong>，因为他证明了贝叶斯定理的一个特例。</p>
<p><img src="http://images.gitbook.cn/3376a8b0-478f-11e8-a985-a3fcb04a2bef" alt="enter image description here" />
<center>托马斯 · 贝叶斯 （1701 – 1761）</center></p>
<p>还有一群统计学家自称“<strong>贝叶斯学派</strong>”，因为他们认为世界是不确定的，这一点和他们的同行冤家“<strong>频率学派</strong>”正好相反。</p>
<p>在<strong>估计概率分布参数</strong>这件事情上，贝叶斯学派和频率学派各自有一套符合自身对世界设想的参数估计方法。</p>
<p>在构造朴素贝叶斯分类器的过程中，当遇到需要求取特征的条件概率分布的时候，我们需要估计该特征对应的条件概率分布的参数。</p>
<p>这时，通常情况下，我们会选用<strong>频率学派的做法——极大似然估计法</strong>。</p>
<p>下面要介绍的，也就是这种方法。</p>
<h3 id="maximumlikelihoodestimationmle">极大似然估计 (Maximum Likelihood Estimation, MLE)</h3>
<p><img src="http://images.gitbook.cn/cf4facb0-4793-11e8-890e-8950f84a2f8b" alt="enter image description here" /></p>
<p>参数估计的<strong>常用策略</strong>是：</p>
<ol>
<li>先假定样本特征具备某种特定的概率分布形式；</li>
<li>再基于训练样本对特征的概率分布参数进行估计。</li>
</ol>
<p>$D_c$ 是训练集中所有被分类为 $c$ 的样本的集合，其中样本数量为 $m_c$；每一个样本都有 $n$ 个特征；每一个特征有一个对应的取值。我们将第 $j$ 个样本的第 $i$ 个特征值记作：$x_i^{(j)}$。</p>
<p>我们已经假设 $P(x_i | c)$ 符合某一种形式的分布，该分布被参数 $\theta_{c,i}$ 唯一确定。为了明确起见，我们把 $P(x_i | c)$ 写作 $P(x_i | \theta_{c,i})$。现在，我们要做的是，利用 $D_c$ 来估计参数 $\theta_{c,i}$。</p>
<p>现在我们要引入一个新的概念——<strong>似然（Likelihood）</strong>。似然指某种事件发生的可能，和概率相似。</p>
<p>二者区别在于：概率用在已知参数的情况下，用来预测后续观测所得到的结果。似然则正相反，用于参数未知，但某些观测所得结果已知的情况，用来对参数进行估计。</p>
<p>参数 $\theta_{c,i}$ 的似然函数记作 $L(\theta_{c,i})$，它表示了 $D_c$ 中的 $m_c$ 个样本 $X_1，X_2，… X_{m_c}$ 在第 $i$ 个特征上的联合概率分布：</p>
<p>$L(\theta_{c,i}) =  \prod_{j=1}^{m_c} P(x_i^{(j)}|\theta_{c,i})$</p>
<p><strong>极大似然估计，就是去寻找让似然函数 $L(\theta_{c,i})$ 的取值达到最大的参数值的估计方法。</strong></p>
<p><img src="http://images.gitbook.cn/ff774010-4793-11e8-b473-a34c15316faa" alt="enter image description here" /></p>
<p>我们把让 $L(\theta_{c,i} ) $ 达到最大的参数值记作 $\theta_{c,i}^*$。则 $\theta_{c,i}^*$ 满足这样的情况：</p>
<ol>
<li><p>将 $\theta_{c,i}^*$ 带入到 $P(x_i|c)$ 的分布形式中去，确定了唯一的一个分布函数 $ f(x)$ （比如这个 $f(x)$ 是一个高斯函数，$\theta_{c,i}^*$ 对应的是它的均值和方差）；</p></li>
<li><p>将 $D_c$ 中每一个样本的第 $i$ 个特征的值带入到 $f(x)$ 中，得到的结果是一个 [0,1] 之间的概率值，将所有 $m_c$ 个 $f(x)$ 的计算结果累计相乘，最后得出的结果是最大的。</p></li>
</ol>
<p>也就是说将参数换成其他任何值，再做上述 1 和 2 之后的结果，一定小于 $\theta_{c,i}^*$ 做 1 和 2 的结果。</p>
<p>说了这么多，我们到底要怎么求取 $\theta_{c,i}^*$ 呢？</p>
<p>求取 $\theta_{c,i}^*$ 的过程，就是最大化 $L(\theta_{c,i})$ 的过程：</p>
<p>$\theta_{c,i}^*= argmax L(\theta_{c,i})$</p>
<p>怎么才能最大化 $L(\theta_{c,i})$ 呢？</p>
<p>为了便于计算，我们对上面等式取对数，得到 $\theta_{c,i}$ 的对数似然：</p>
<p>$LL(\theta_{c,i}) = \sum_{j=1}^{m_c} log(P(x_i^{(j)}|\theta_{c,i})) $</p>
<p>要知道，<strong>最大化一个似然函数同最大化它的自然对数是等价的</strong>。</p>
<p>因为自然对数 log 是一个连续且在似然函数的值域内严格递增的上凸函数。所以我们可以参考前面线性回归目标函数的求解办法：对似然函数求导，然后在设定导函数为0的情况下，求取 $\theta_{c,i}$ 的最大值——$\theta_{c,i}^*$</p>
<h3 id="-2">正态分布的极大似然估计</h3>
<p><img src="http://images.gitbook.cn/d1a435f0-4791-11e8-9a2d-c1e768ea8f10" alt="enter image description here" /></p>
<p>下面来看一个具体的例子——<strong>$P(x_i | c)$ 为正态分布</strong>。此时，我们有：</p>
<p>$LL(\theta_{c,i}) =  LL(\mu_{c,i}, \sigma_{c,i}^2) = \sum_{j=1}^{m_c} log(\frac{1}{{ \sqrt {2\pi \sigma_{c,i}^2 } }}exp( \frac{-(x_i^{(j)} - \mu_{c,i})^2 }{{2\sigma_{c,i} ^2 }})) $</p>
<p>注意，这里我们把 $\sigma_{c,i}^2$ 看作一个独立参数，即：</p>
<p>$\theta_{c,i} = (\mu_{c,i}, \sigma_{c,i} ^ 2)$</p>
<p>我们先对 $\mu_{c,i}$ 求偏导：</p>
<p>$\frac{\partial{ LL(\mu_{c,i}, \sigma_{c,i}^2)}}{\partial{\mu_{c,i}}} = \sum_{j=1}^{m_c}( \frac{x_i^{(j)} - \mu_{c,i} }{{\sigma_{c,i} ^2 }})  $</p>
<p><strong>令：</strong> $\frac{\partial{ LL(\mu_{c,i}, \sigma_{c,i}^2)}}{\partial{\mu_{c,i}}} = 0 $</p>
<p><strong>则：</strong> $\sum_{j=1}^{m_c}( \frac{x_i^{(j)} - \mu_{c,i} }{{\sigma_{c,i} ^2 }}) = 0 $</p>
<p><strong>有：</strong> $\mu_{c,i} = \frac{1}{m_c} \sum_{j=1}^{m_c}(x_i^{(j)})  $ </p>
<p>然后对 $\sigma_{c,i}$ 求偏导：</p>
<p>$\frac{\partial{ LL(\mu_{c,i}, \sigma_{c,i}^2)}}{\partial{\sigma_{c,i}^2}} = \sum_{j=1}^{m_c}(\frac{-1}{2\sigma_{c,i}^2} + \frac{(x_i^{(j)} - \mu_{c,i})^2}{2\sigma_{c,i}^2\sigma_{c,i}^2}) $</p>
<p><strong>令：</strong> $\sum_{j=1}^{m_c}(\frac{-1}{2\sigma_{c,i}^2} + \frac{(x_i^{(j)} -\mu_{c,i})^2}{2\sigma_{c,i}^2\sigma_{c,i}^2}) = 0 $</p>
<p>最后得出：</p>
<p>$\sigma_{c,i}^2 = \frac{1}{m_c} \sum_{j=1}^{m_c}(x_i^{(j)} - \mu_{c,i})^2 $</p>
<p>这样我们就估算出了第 $i$ 个特征正态分布的两个参数，第 $i$ 个特征的具体分布也就确定下来了。</p>
<p>注意：虽然我们此处用的是正态分布——一种连续分布，但是实际上，离散特征的分布一样可以用同样的方法来求，只不过把高斯分布公式改成其他的分布公式就好了。</p>
<p>估算出了每一个特征的 $\theta_{c,i}$ 之后，再将所有求出了具体参数的分布带回到朴素贝叶斯公式中，生成朴素贝叶斯分类器，再用来做预测，就好了。</p>
<h3 id="-3">用代码实现朴素贝叶斯模型</h3>
<p>上文中<strong>例子4</strong>用代码来实现，如下所示：</p>
<pre><code>    import pandas as pd
    import numpy as np
    import time
    from sklearn.model_selection import train_test_split
    from sklearn.naive_bayes import GaussianNB

    # Importing dataset. 
    # Please refer to the 【Data】 part after the code for the data file.
    data = pd.read_csv("career_data.csv") 

    # Convert categorical variable to numeric
    data["985_cleaned"]=np.where(data["985"]=="Yes",1,0)
    data["education_cleaned"]=np.where(data["education"]=="bachlor",1,
                                      np.where(data["education"]=="master",2,
                                               np.where(data["education"]=="phd",3,4)
                                              )
                                     )
    data["skill_cleaned"]=np.where(data["skill"]=="c++",1,
                                      np.where(data["skill"]=="java",2,3
                                              )
                                     )
    data["enrolled_cleaned"]=np.where(data["enrolled"]=="Yes",1,0)

    # Split dataset in training and test datasets
    X_train, X_test = train_test_split(data, test_size=0.1, random_state=int(time.time()))

    # Instantiate the classifier
    gnb = GaussianNB()
    used_features =[
        "985_cleaned",
        "education_cleaned",
        "skill_cleaned"
    ]

    # Train classifier
    gnb.fit(
        X_train[used_features].values,
        X_train["enrolled_cleaned"]
    )
    y_pred = gnb.predict(X_test[used_features])

    # Print results
    print("Number of mislabeled points out of a total {} points : {}, performance {:05.2f}%"
          .format(
              X_test.shape[0],
              (X_test["enrolled_cleaned"] != y_pred).sum(),
              100*(1-(X_test["enrolled_cleaned"] != y_pred).sum()/X_test.shape[0])
    ))
</code></pre>
<p>下列数据直接存储为 career_data.csv 文件：</p>
<blockquote>
  <p>no,985,education,skill,enrolled </p>
  <p>1,Yes,bachlor,C++,No </p>
  <p>2,Yes,bachlor,Java,Yes </p>
  <p>3,No,master,Java,Yes </p>
  <p>4,No,master,C++,No</p>
  <p>5,Yes,bachlor,Java,Yes </p>
  <p>6,No,master,C++,No </p>
  <p>7,Yes,master,Java,Yes</p>
  <p>8,Yes,phd,C++,Yes </p>
  <p>9,No,phd,Java,Yes</p>
  <p>10,No,bachlor,Java,No</p>
</blockquote>
<p>数据和脚本放在统一路径下，运行脚本，输出如下：</p>
<blockquote>
  <p>Number of mislabeled points out of a total 1 points : 0, performance 100.00%</p>
</blockquote>
<h3 id="-4">参考资料</h3>
<p>有人用50行代码就完成朴素贝叶斯分类器的训练和预测全过程，感兴趣的可以看看：<a href="https://ebiquity.umbc.edu/blogger/2010/12/07/naive-bayes-classifier-in-50-lines/">《Naive Bayes classifier in 50 lines》</a>。</p></div></article>