---
title: 机器学习极简入门-16
---
<article id="topicContainer" class="column_content"><h2 class="topic_title"></h2><div><p>现在我们回到 LR 模型本身。</p>
<h3 id="">回归模型做分类</h3>
<p>从前面关于分类与回归的定义来看，分类模型和回归模型似乎是泾渭分明的。输出离散结果的就是用来做分类的，而输出连续结果的，就用来做回归。</p>
<p>我们前面讲的两个模型：线性回归的预测结果是一个连续值域上的任意值，而朴素贝叶斯分类模型的预测结果则是一个离散值。</p>
<p>但 LR 却是用来做分类的。它的模型函数为：</p>
<p>$h_\theta(x) = \frac{1}{1 + e^{-\theta^Tx }} $</p>
<p>设 $z = \theta^T x$，则 </p>
<p>$h(z) = \frac{1}{1 + e^{-z }} $</p>
<p>在二维坐标中形成 S 形曲线：</p>
<p><img src="http://images.gitbook.cn/def9a1f0-5285-11e8-804e-bb449981bb15" alt="enter image description here" /></p>
<p>上图中，$z$ 是自变量（横轴），最终计算出的因变量 $y$（纵轴），则是一个 [0,1] 区间之内的实数值。</p>
<p>一般而言，当 $y&gt;0.5$ 时，$z$ 被归类为真（True）或阳性（Positive），否则当 $y &lt;=0.5$ 时，$z$ 被归类为假（False）或阴性（Negative）。</p>
<p>所以，在模型输出预测结果时，不必输出 $y$ 的具体取值，而是根据上述判别标准，输出1（真）或0（假）。</p>
<p>因此，LR 典型的应用是二分类问题上，也就是说，把所有的数据只分为两个类。</p>
<p><strong>注意：</strong> 当然，这并不是说 LR 不能处理多分类问题，它当然可以处理，具体方法稍后讲。我们先来看 LR 本身。</p>
<p>看到此处，大家是不是会有点担心，如果大量的输入得到的结果都在 $y=0.5$ 附近，那岂不是很容易分错？</p>
<p>说得极端一点，如果所有的输入数据得出的结果都在 $y=0.5$ 附近，那岂不是没有什么分类意义了，和随机乱归类结果差不多？</p>
<p>这样的担心其实是不必要的。此模型函数在 $y=0.5$ 附近非常敏感，自变量取值稍有不同，因变量取值就会有很大差异，所以不用担心出现大量因细微特征差异而被归错类的情况——这也正是逻辑回归的“神奇”之处。</p>
<h3 id="-1">逻辑回归的目标函数</h3>
<p>有了模型函数，来看看逻辑回归的目标函数。</p>
<p>逻辑函数 $h(x)$ 是我们要通过训练得出来的最终结果。在最开始的时候，我们不知道其中的参数 $\theta$ 的取值，我们所有的只是若干的 $x$ 和与其对应的 $y$（训练集合）。训练 LR 的过程，就是求 $\theta$ 的过程。</p>
<p>首先要设定一个目标：我们希望这个最终得出的 $\theta$ 达到一个什么样的效果——我们当然是希望得出来的这个 $\theta$，能够让训练数据中被归为阳性的数据预测结果都为阳，本来被分为阴性的预测结果都为阴。</p>
<p>而从公式本身的角度来看，$h(x)$ 实际上是 $x$ 为阳性的分布概率，所以，才会在 $h(x) &gt; 0.5$ 时将 $x$ 归于阳性。也就是说 $h(x) = P(y=1)$。反之，样例是阴性的概率 $P(y=0) = 1 - h(x)$。</p>
<p>当我们把测试数据带入其中的时候，$P(y=1)$ 和 $P(y=0)$ 就都有了先决条件，它们为训练数据的 $x$ 所限定。因此：</p>
<p>$P(y=1|x) = h(x); P(y=0|x) = 1- h(x)$。</p>
<p>根据<strong>二项分布</strong>公式，可得出 $P(y|x) = h(x) ^y(1- h(x))^{(1-y)}$。</p>
<p>假设我们的训练集一共有 m 个数据，那么这 m 个数据的联合概率就是：</p>
<p>$L(\theta) =  \prod_{i=1}^{m}P(y^{(i)}|x^{(i)};\theta) = \prod_{i=1}^{m}(h_\theta(x^{(i)}))^{y^{(i)}}(1-h_\theta(x^{(i)}))^{(1-y^{(i)})}$</p>
<p>我们求取 $\theta$ 的结果，就是让这个 $L(\theta)$ 达到最大。</p>
<p>还记得我们之前在朴素贝叶斯分类器中讲到的<strong>极大似然估计</strong>吗？其实此处 LR 目标函数的构建过程也是依据极大似然估计。</p>
<p>$L(\theta)$ 就是 LR 的<strong>似然函数</strong>。我们要让它达到最大，也就是对其进行“<strong>极大估计</strong>”。因此，求解 LR 目标函数的过程，就是对 LR 模型函数进行极大似然估计的过程。</p>
<p>为了好计算，我们对它求对数。得到<strong>对数似然函数</strong>：</p>
<p>$    l(\theta) =  \log(L(\theta)) = \sum_{i=1}^{m}[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$</p>
<p>我们要求出让 $l(\theta)$ 能够得到最大值的 $\theta$。</p>
<p>$l(\theta)$ 其实可以作为 LR 的目标函数。前面讲过，我们需要目标函数是一个凸函数，具备最小值。因此我们设定：$J(\theta) = -l(\theta) $。</p>
<p>$    J(\theta) = - \log(L(\theta)) = -\sum_{i=1}^{m}[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$</p>
<p>这样，求 $l(\theta)$ 的最大值就成了求 $J(\theta)$ 的最小值。$J(\theta)$ 又叫做<strong>负对数似然函数</strong>。它就是 <strong>LR 的目标函数</strong>。</p>
<h4 id="-2">优化算法</h4>
<p>我们已经得到了 <strong>LR 的目标函数 $J(\theta)$</strong>，并且<strong>优化目标是最小化它</strong>。</p>
<p>如何求解 $\theta$ 呢？具体方法其实有很多。此处我们仍然运用之前已经学习过的，最常见最基础的梯度下降算法。</p>
<p>基本步骤如下：</p>
<p>•    通过对 $J(\theta)$ 求导获得下降方向—— $J ' (\theta) $；</p>
<p>•    根据预设的步长 $\alpha$，更新参数 $\theta := \theta − \alpha  J’(θ)$；</p>
<p>•    重复以上两步直到逼近最优值，满足终止条件。</p>
<p><img src="http://images.gitbook.cn/f048fa90-5286-11e8-bcd6-e300dcfa6492" alt="enter image description here" /></p>
<p>既然知道了方法，我们就来计算一下。</p>
<p>已知：</p>
<p>$    J(\theta) = - \log(L(\theta)) = -\sum_{i=1}^{m}[y^{(i)}\log(h_\theta(x^{(i)})) + (1-y^{(i)})\log(1-h_\theta(x^{(i)}))]$</p>
<p>$J(\theta)$ 对 $\theta$ 求导：</p>
<p>$\frac{\partial{ J(\theta)}}{\partial{\theta}}  = -\sum_{i=1}^{m}[y^{(i)}\frac{h_\theta'(x^{(i)})}{h_\theta(x^{(i)})} - (1-y^{(i)})\frac{h_\theta'(x^{(i)})}{(1-h_\theta(x^{(i)}))}]  = \sum_{i=1}^{m}[(-y^{(i)})\frac{h_\theta'(x^{(i)})}{h_\theta(x^{(i)})}+(1-y^{(i)})\frac{h_\theta'(x^{(i)})}{(1-h_\theta(x^{(i)}))}] $</p>
<p>因为有：</p>
<p>$h'(z) = \frac{d(\frac{1}{1+e^{-z}})}{dz} = -(\frac{-e^{-z}}{(1 + e^{-z})^2}) = \frac{e^{-z}}{1+e^{-z}}\frac{1}{1+e^{-z}} = (1- \frac{1}{1+e^{-z}})(\frac{1}{1+e^{-z}} ) = h(z)(1 - h(z))$</p>
<p>同时，运用链式法则，有：</p>
<p>$\frac{\partial{ h_\theta(x)}}{\partial{\theta}} = \frac{\partial{ h_\theta(x)}}{\partial{(\theta x)}} x = h_\theta(x)(1 -  h_\theta(x))x  $</p>
<p>将上式带入上面的 $J(\theta)$ 求导式子里，有：</p>
<p>$\frac{\partial{ J(\theta)}}{\partial{\theta}}  =  \sum_{i=1}^{m}[(-y^{(i)})\frac{h_\theta(x^{(i)})(1- h_\theta(x^{(i)}))x^{(i)}}{h_\theta(x^{(i)})} + (1-y^{(i)})\frac{h_\theta(x^{(i)})(1- h_\theta(x^{(i)}))x^{(i)}}{(1-h_\theta(x^{(i)}))}]  =  \sum_{i=1}^{m}[-y^{(i)} + y^{(i)}h_\theta(x^{(i)})  + h_\theta(x^{(i)}) - y^{(i)}h_\theta(x^{(i)}) ]x^{(i)} =  \sum_{i=1}^{m}[ h_\theta(x^{(i)}) -y^{(i)}]x^{(i)}$</p>
<p>当 $x$ 为多维的时候（设 $x$ 有 $n$ 维），则在对 $z=\theta x$ 求导的时候，要对 $x$ 的每一个维度求导。</p>
<p>又因为 $\theta$ 和 $x$ 维度相同，所以当 $x$ 有 $n$ 维的时候，$\theta$ 同样是有 $n$ 维的。则 $J(\theta)$ 的求导也变成了对 $\theta$ 的每一个维度求导：</p>
<p>$\frac{\partial{ J(\theta)}}{\partial{\theta_j}}  =  \sum_{i=1}^{m}[ h_\theta(x^{(i)}) -y^{(i)}]x_j^{(i)} ;\quad j = 1, 2, ..., n$</p>
<p>因此，优化算法伪代码为：</p>
<blockquote>
  <p>Set initial value: $\theta_0, \alpha$</p>
  <p>while (not convergence)</p>
  <p>{</p>
  <p>$\qquad \theta_j := \theta_j + \alpha\sum_{i=1}^{m}( y^{(i)} -  h_\theta(x^{(i)}))x_j^{(i)}$</p>
  <p>}</p>
</blockquote>
<h3 id="-3">实例及代码实现</h3>
<p>我们来看一个例子，比如某位老师想用学生上学期考试的成绩（Last Score）和本学期在学习上花费的时间（Hours Spent）来预期本学期的成绩：</p>
<p><img src="http://images.gitbook.cn/8884fe80-5287-11e8-ae90-8538fe442b90" alt="enter image description here" /></p>
<p>面对这样一个需求，我们可能首先想到的是线性回归，毕竟，要做的是预测本次的成绩。那样的话，我们取 X = [“Last Score”, “Hours Spent”]，y = “Score”。</p>
<p>用线性回归实现代码如下：</p>
<pre><code>    from sklearn.linear_model import LogisticRegression
    from sklearn.linear_model import LinearRegression
    import pandas as pd

    # Importing dataset
    data = pd.read_csv('quiz.csv', delimiter=',')        
    used_features = ["Last Score", "Hours Spent"]
    X = data[used_features].values
    scores = data["Score"].values

    X_train = X[:11]
    X_test = X[11:]

    # Linear Regression - Regression
    y_train = scores[:11]
    y_test = scores[11:]

    regr = LinearRegression()
    regr.fit(X_train, y_train)
    y_predict = regr.predict(X_test)

    print(y_predict)
</code></pre>
<p>我们把前11个样本作为训练集，最后3个样本作为测试集。</p>
<p>这样训练出来之后，得到的预测结果为：[55.33375602 54.29040467 90.76185124]，也就说 id 为 12、13、14 的三个同学的预测分数为55、54和91。</p>
<p>第一个差别比较大，id 为12的同学，明明考及格了，却被预测为不及格。</p>
<p>这是为什么呢？大家注意 id 为4的同学，这是一位学霸，他只用了20小时在学习上，却考出了第一名的好成绩。</p>
<p>回想一下线性回归的目标函数，我们不难发现，所有训练样本对于目标的贡献是平均的，因此，4号同学这种超常学霸的出现，在数据量本身就小的情况下，有可能影响整个模型。</p>
<p>这还是幸亏我们有历史记录，知道上次考试的成绩，如果 X 只包含“Hours Spent”，学霸同学根本就会带偏大多数的预测结果（自变量只有“Hours Spent”的线性回归模型会是什么样的？这个问题留给同学们自己去实践）。</p>
<p>那么我们看看用逻辑回归如何。用逻辑回归的时候，我们就不再是预测具体分数，而是预测这个学生本次能否及格了。</p>
<p>这样我们就需要对数据先做一下转换，把具体分数转变成是否合格，合格标志为1，不合格为0，然后再进行逻辑回归：</p>
<pre><code>    from sklearn.linear_model import LogisticRegression
    from sklearn.linear_model import LinearRegression
    import pandas as pd

    # Importing dataset
    data = pd.read_csv('quiz.csv', delimiter=',')

    used_features = [ "Last Score", "Hours Spent"]
    X = data[used_features].values
    scores = data["Score"].values

    X_train = X[:11]
    X_test = X[11:]

    # Logistic Regression – Binary Classification
    passed = []

    for i in range(len(scores)):
        if(scores[i] &gt;= 60):
            passed.append(1)
        else:
            passed.append(0)

    y_train = passed[:11]
    y_test = passed[11:]

    classifier = LogisticRegression(C=1e5)
    classifier.fit(X_train, y_train)

    y_predict = classifier.predict(X_test)
    print(y_predict)
</code></pre>
<p>这次的输出就是[1 0 1]，对12、13、14号同学能否通过本次考试的判断是正确的。</p>
<h3 id="lr">LR 处理多分类问题</h3>
<p>LR 是用来做二分类的，但是如果我们面对的是多分类问题：样本标签的枚举值多于2个，还能用 LR 吗？</p>
<p>当然是可以的。我们可以把二分类问题分成多次来做。</p>
<p>假设你一共有 n 个标签（类别），也就是说可能的分类一共有 n 个。那么就构造 n 个 LR 分类模型，第一个模型用来区分 <code>label_1</code>和 <code>non-label _1</code>（即所有不属于 <code>label_1</code> 的都归属到一类），第二个模型用来区分 <code>label_2</code> 和 <code>non-label _2</code>……, 第 n 个模型用来区分 <code>label_n</code> 和 <code>non-label _n</code>。</p>
<p>使用的时候，每一个输入数据都被这 n 个模型同时预测。最后哪个模型得出了 Positive 结果，就是该数据最终的结果。</p>
<p>如果有多个模型都得出了 Positive，那也没有关系。因为 LR 是一个回归模型，它直接预测的输出不仅是一个标签，还包括该标签正确的概率。那么对比几个 Positive 结果的概率，选最高的一个就是了。</p>
<p>例如，有一个数据，第一和第二个模型都给出了 Positive 结果，不过  <code>label_1</code> 模型的预测值是0.95，而 <code>label_2</code> 的结果是0.78，那么当然是选高的，结果就是 <code>label_1</code>。</p>
<p>说起原理来好像挺麻烦，好在 sklearn 已经为我们处理了多分类问题，我们用 sklearn 来做多分类的时候，只是需要把 y 准备好，其他的，都和做二分类一样就可以了。</p>
<p>比如还是上面的例子，现在我们需要区分：学生的本次成绩是优秀（&gt;=85），及格，还是不及格。我们就在处理 y 的时候给它设置三个值：0 （不及格）、1（及格）和2（优秀），然后再做 LR 分类就可以了。代码如下：</p>
<pre><code>    from sklearn.linear_model import LogisticRegression
    from sklearn.linear_model import LinearRegression
    import pandas as pd

    # Importing dataset
    data = pd.read_csv('quiz.csv', delimiter=',')

    used_features = [ "Last Score", "Hours Spent"]
    X = data[used_features].values
    scores = data["Score"].values

    X_train = X[:11]
    X_test = X[11:]

    # Logistic Regression - Multiple Classification
    level = []

    for i in range(len(scores)):
        if(scores[i] &gt;= 85):
            level.append(2)
        elif(scores[i] &gt;= 60):
            level.append(1)
        else:
            level.append(0)

    y_train = level[:11]
    y_test = level[11:]

    classifier = LogisticRegression(C=1e5)
    classifier.fit(X_train, y_train)

    y_predict = classifier.predict(X_test)
    print(y_predict)
</code></pre>
<p>测试集的输出是：[1 0 2] —— 12号及格，13号不及格，14号优秀，还是蛮准的。</p>
<h3 id="-4">附录</h3>
<p>quiz.csv 文件：</p>
<blockquote>
  <p>Id,Last Score,Hours Spent,Score </p>
  <p>1,90,117,89 </p>
  <p>2,85,109,78 </p>
  <p>3,75,113,82</p>
  <p>4,98,20,95 </p>
  <p>5,62,116,61 </p>
  <p>6,36,34,32 </p>
  <p>7,87,120,88 </p>
  <p>8,89,132,92 </p>
  <p>9,60,83,52</p>
  <p>10,72,92,65 </p>
  <p>11,73,112,71 </p>
  <p>12,56,143,62 </p>
  <p>13,57,97,52 </p>
  <p>14,91,119,93</p>
</blockquote></div></article>