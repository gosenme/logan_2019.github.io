---
title: 攻克 Linux 系统编程-3
---
<article id="topicContainer" class="column_content"><h2 class="topic_title"></h2><div><p>在 Linux 中有条重要的哲学，即一切皆文件，本文就来<strong>着重讲讲文件操作</strong>。首先从磁盘文件开始，探讨除打开、读、写、关闭等常规操作外，还有哪些可控操作。理解和掌握各个步骤的行为细节，可以<strong>帮助开发者写出性能更好、更加稳健的应用</strong>。本文主要包括以下几部分内容：</p>
<ul>
<li>默认的文件读写行为</li>
<li>在应用层选择合适的文件 IO 缓存</li>
<li>控制文件内容在磁盘设备上的更新</li>
<li>控制文件内容的预读取策略</li>
<li>混合使用库函数和系统调用</li>
</ul>
<h3 id="21">2.1 默认的文件读写行为</h3>
<p>要想更精准地控制文件读写行为，首先要弄清楚，在不做任何额外控制的情况下，系统默认动作是什么。了解了这些，我们才能知道默认行为适合哪些情况，而在哪些情况下需要我们介入附加控制。</p>
<h4 id="211">2.1.1 文件读写默认操作</h4>
<p>我们首先分别看下文件读取、文件写入的系统默认动作。</p>
<ul>
<li><p><strong>先说文件读取</strong></p>
<p>默认情况下，当使用 read 系统调用从文件中读取一些字节时，Linux 内核除了读取指定字节数的数据外，还会额外预读取一些数据到内核缓冲区。下次再读取文件内容时，会先从内核缓冲区中查找，如果正好找到了，则省去了等待慢速磁盘定位和数据传输的时间。在大多数 Linux 系统中，预读取数据的长度为 128 KB，也可能根据系统可用内存的大小动态调整。</p></li>
<li><p><strong>再看文件写入</strong></p>
<p>当用 write 系统调用写入文件内容时，函数将数据回写到内核缓冲区之后便返回，Linux 内核负责在稍后一段时间内将文件内容真正写入到磁盘中。除了更新文件本身的内容，还会更新文件的元数据，如文件大小、文件关联的 inode、文件最后修改时间等信息。</p></li>
</ul>
<h4 id="212">2.1.2 特殊读写处理</h4>
<p>默认的文件读写操作已能很好地满足绝大多数应用的需求，但仍<strong>有一些特殊的读写操作需做特别处理</strong>。</p>
<p><strong>比如</strong>，某些应用对数据可靠性有很高要求，某段代码逻辑需保证文件内容确实已经更新到磁盘上后，才会向下运行。显然，默认的写操作（内容会在稍后一段时间内由操作系统内核写入磁盘）是无法满足这一需求的。</p>
<p><strong>还有</strong>，某些应用在读取数据时有固定的访问模式，比如读取某段数据之后，后续肯定会再读取文件某偏移处的数据，这时需针对该固定访问模式做深度优化。</p>
<p><strong>还有一些数据库应用</strong>，已在应用层实现了自己的数据管理策略，当使用 write 系统调用时，希望系统将数据直接写入磁盘，无需缓冲数据，进而节省内核资源。</p>
<p>另外，<strong>还有一点需要注意</strong>，系统调用在内核中已经实现了一套高速缓存，但这并不意味着，应用可以随意执行系统调用，而不用关心性能问题。倘若每次使用系统调用读取或写入的字节数很少，系统调用本身的开销将占用总开销很大比例。因此，在应用层上加一层数据缓存，以尽量减少系统调用的次数，可明显提升应用的性能表现。</p>
<p>本文接下来依次讨论各个层次上的那些可控制选项。</p>
<h3 id="22io">2.2 选取合适的文件 IO 缓存</h3>
<p>先从应用层开始，谈谈应用层缓存问题。</p>
<p>前面提到，应用层应该加一层缓存，以尽量减少系统调用的使用次数，以此来提高应用的整体性能表现。那么该缓存大小应该如何确定呢？</p>
<p>在某文献中有这样一个测试，以 Ext2 文件系统中的常规文件为例，当该文件系统的块大小为 4K 字节时，在应用层设置 4K 字节的缓存，相比单字节调用系统调用，性能可以提升两个数量级。进一步增大应用层的缓存大小，性能不再有明显提升。基于这个测试结果，我们可以得出，<strong>应用层缓存大小至少应该等于所使用文件系统的块大小</strong>。</p>
<p>实际上，还有一个获取文件信息的系统调用 stat。其获得的文件属性信息中，有一项建议了文件 IO 缓存大小，低于此值的缓存大小会被认为是低效的。其函数原型为：</p>
<pre><code>int stat(const char * pathname, struct stat *statbuf);
</code></pre>
<p>这一项正是 struct stat 结构体中的 st_blksize 字段。设置应用层的缓存大小，至少不小于该字段给出的数值。</p>
<p>此外，<strong>glibc 中提供的 fread 和 fwrite 函数，其内部都维护了一个数据缓存，用来尽量减少系统调用次数</strong>。默认选择的缓存大小已进行了充分优化。如果还是不满意，可以用 glibc 的 setvbuf 和 setbuffer 函数自定义缓存大小和缓存行为。这两个函数的原型分别为：</p>
<pre><code>int setvbuf(FILE *stream, char *buf, int mode, size_t size);
void setbuffer(FILE *stream, char *buf, size_t size);
</code></pre>
<p>其中的 setvbuf 函数允许开发者指定缓冲方式，主要有以下三种可选方式。</p>
<ul>
<li>_IONBF：不缓冲，标准错误输出默认选择该缓冲方式，以保证错误信息及时输出来。</li>
<li>_IOLBF：行缓冲，也就是遇到换行符时，对之前的内容执行 read、write 系统调用，终端设备默认执行该缓冲方式。</li>
<li>_IOFBF：全缓冲，也叫块缓冲，当指定大小的缓冲区满了之后，才会触发调用一次系统调用，磁盘文件默认使用该缓冲方式。同时，glibc 还提供了 fflush 函数，应用可以在缓冲区数据满之前，手动将数据刷新到内核缓冲区。</li>
</ul>
<p><font color="#F39800">出于性能上的考虑，读写磁盘文件应该使用 fread 和 fwrite 函数，而不是直接使用 read 和 write 系统调用</font>。同时，可以使用库函数默认的缓冲区，也可以根据 stat 的建议设置合适大小的自定义缓冲区。</p>
<h3 id="23">2.3 文件内容在磁盘上的更新</h3>
<p>知道了如何设置缓冲区，下面就来说说如何控制数据在磁盘上的更新。</p>
<p>在 Linux 内核中，文件数据的磁盘同步状态有两个层次，分别为“同步 IO 数据完整性”和“同步 IO 文件完整性”，名称有点长，后面我们简称为“数据完整性”和“文件完整性”。</p>
<ul>
<li><strong>数据完整性</strong>是指文件的内容数据已经写入到磁盘中了。</li>
<li><strong>文件完整性</strong>指的是不止文件的内容数据，文件的元数据也已写入磁盘中。</li>
</ul>
<p>文件的元数据是指描述文件信息的数据，如文件创建时间、大小、占用节点数据等。文件完整性包含了数据完整性。还有一点需要明确，文件的元数据和文件数据并未保存在同一磁盘位置上。</p>
<h4 id="231fdatasyncfsync">2.3.1 fdatasync 和 fsync 系统调用</h4>
<p>Linux 提供了 fdatasync 和 fsync 两个系统调用，分别用来保证数据完整性和文件完整性。函数原型分别为：</p>
<pre><code>int fdatasync(int fd);
int fsync(int fd);
</code></pre>
<p>所以，前面提到的对数据可靠性要求很高的应用，就可以在 write 之后调用 fdatasync，强制将数据从内核缓冲区刷新到磁盘中。</p>
<p>当然也可以调用 fsync ，更加彻底地把数据元数据信息也刷新到磁盘中去。</p>
<p>不过大家需要清楚，这样做会带来<strong>更高的操作延迟</strong>，因为文件的元数据通常保存在不同的磁盘位置上，需多花一些寻道时间来保存元数据。具体使用哪种层次的数据同步，可以根据应用需求灵活选择。</p>
<h4 id="232o_dsynco_sync">2.3.2 O_DSYNC 和 O_SYNC 标志</h4>
<p>除此之外，使用 open 打开文件时指定 O_DSYNC 或 O_SYNC 标志，也可以让 write 系统调用在该文件上写入数据时，分别达到数据完整性、文件完整性同步状态，即让 write 系统调用在文件上的表现等同于 write + fdatasync 或 write + fsync。但是，<strong>使用这两个标志会影响到在该文件上的每一次 write 系统调用的执行</strong>。</p>
<blockquote>
  <p>编码时，推荐根据具体需求谨慎选择调用 fdatasync 或是 fsync。不建议使用打开标志，该方式会损失一些灵活性与性能。</p>
</blockquote>
<h3 id="24">2.4 控制文件内容的预读取策略</h3>
<p>如何根据数据访问顺序的特定模式深入优化应用，针对这个问题，Linux 内核提供了 posix_fadvise 系统调用。它允许应用程序告知内核访问某个文件数据时采取的模式。系统调用原型为：</p>
<pre><code>int posix_fadvise(int fd, off_t offset, off_t len, int advice);
</code></pre>
<p>其中，advice 参数指定了所采用的模式，支持的模式有以下几种。</p>
<ul>
<li>POSIX_FADV_NORMAL：默认模式，内核会把文件预读窗口设置为默认值（128K）。</li>
<li>POSIX_FADV_SEQUENTAL：顺序访问模式，内核会把文件预读窗口设置为默认值的两倍。</li>
<li>POSIX_FADV_RANDOM：完全随机访问数据，在这种模式下，内核不使用文件预读。</li>
<li>POSIX_FADV_WILLNEED：在这种模式下，应用可以同时提供 offset 和 len 参数，指示接下来要使用的文件的位置和数据长度。根据这两个参数，内核会将数据预读到内核缓冲区。</li>
<li>POSIX_FADV_DONTNEED：在这种模式下，应用同样可以提供 offset 和 len 参数，但指示的是接下来不会使用的文件的位置和数据长度，内核会释放这部分数据所占用的内核缓冲区。</li>
<li>POSIX_FADV_NOREUSE：这种模式指定的文件区域，会在接下来仅访问一次。在下次被读取后，相应的缓存页面会从缓冲区中释放掉。</li>
</ul>
<p>灵活使用这些参数，可以帮助内核对应用采取更加友好的行为策略，或者可以提高缓存的命中率，从而提高性能表现，或者可以节省内核所占用的宝贵内存资源。</p>
<p>最后，再来谈谈不使用内核缓冲区，<strong>直接与磁盘进行数据传输</strong>的问题。在 Linux 中，将这一过程称为直接 IO，实现方式是在打开文件的同时指定 O_DIRECT 标志。</p>
<p>使用这种方式写入文件时，内存边界需要是文件系统块大小的整数倍，传递数据的长度也需要是块大小的整数倍。这时，可能需要使用 memalign 此类技术来分配数据内存块。详细信息，请参看 Linux 手册，或自行 Google 查找专门的介绍材料，这里就不展开了。</p>
<h3 id="25glibc">2.5 混合使用 glibc 函数与系统调用</h3>
<p>经过上面的解说，我们已经知道如何应对某些对文件 IO 有特殊要求的需求了。不过，似乎还有最后一个问题有待解决。</p>
<p>前面讨论过，为了尽量减少系统调用的次数，我们推荐使用 glibc 的 fread 和 fwrite 函数操作文件，这两个函数需要的参数是 FILE 类型。</p>
<p>如果我们既希望使用系统调用控制数据同步和内核缓冲行为，又需要使用以整数型为参数的系统调用，该如何混合使用两者操作同一个文件呢？</p>
<p>其实，C 标准库提供了实现两者间互相转换的函数：</p>
<pre><code>int fileno(FILE *fp)
FILE * fdopen(int fd, const char * mode)
</code></pre>
<p>其中 fdopen 中的文件模式需要和 open 打开文件时的模式相同，否则会失败。</p>
<p>终于，最后一块拼图也完整了。</p>
<h3 id="26">2.6 总结</h3>
<p>本文深入讨论了除<strong>常规打开、读写、关闭</strong>之外，对磁盘文件还可进行的<strong>其他更高级的控制功能</strong>，以及<strong>适用场景</strong>。希望它们能成为读者工具箱中的一部分，变成打磨读者自己应用的利器。</p>
<p>点击了解：<a href="https://gitbook.cn/gitchat/column/5bfbbe9b7d496f13396961de?utm_source=ywtsd001">《攻克 Linux 系统编程》</a></p></div></article>